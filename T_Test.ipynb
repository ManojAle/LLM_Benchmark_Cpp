{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Define function to normalize column names\n",
        "def normalize_columns(df):\n",
        "    column_mapping = {\n",
        "        'Precision_Non_performance': 'Precision_Non_Performance',\n",
        "        'Recall_Non_performance': 'Recall_Non_Performance',\n",
        "        'F1 Score_Non_Performance': 'F1_Score_Non_Performance',\n",
        "        'Precision_Performance': 'Precision_Performance',\n",
        "        'Recall_Performance': 'Recall_Performance',\n",
        "        'F1 Score_Performance': 'F1_Score_Performance',\n",
        "    }\n",
        "    return df.rename(columns=lambda col: column_mapping.get(col, col))\n",
        "\n",
        "# Define function to check and fill missing columns\n",
        "def check_and_fill_columns(df, expected_columns):\n",
        "    for col in expected_columns:\n",
        "        if col not in df.columns:\n",
        "            print(f\"Warning: Column '{col}' is missing in the dataset. Filling with NaNs.\")\n",
        "            df[col] = float('nan')  # Add missing column filled with NaN\n",
        "    return df\n",
        "\n",
        "# Define function to perform paired t-test\n",
        "def perform_paired_ttest(model1_metrics, model2_metrics):\n",
        "    results = {'performance': {}, 'non_performance': {}}\n",
        "\n",
        "    # Paired t-test for Performance class metrics\n",
        "    for metric in ['Precision_Performance', 'Recall_Performance']:\n",
        "        t_stat, p_value = stats.ttest_rel(model1_metrics[metric], model2_metrics[metric])\n",
        "        results['performance'][metric] = {'t_stat': t_stat, 'p_value': p_value}\n",
        "\n",
        "    # Paired t-test for Non-Performance class metrics\n",
        "    for metric in ['Precision_Non_Performance', 'Recall_Non_Performance']:\n",
        "        t_stat, p_value = stats.ttest_rel(model1_metrics[metric], model2_metrics[metric])\n",
        "        results['non_performance'][metric] = {'t_stat': t_stat, 'p_value': p_value}\n",
        "\n",
        "    return results\n",
        "\n",
        "# File paths\n",
        "file_paths = {\n",
        "    \"CodeBert_KD_M\": \"CodeBert_KD_M.csv\",\n",
        "    \"CodeBert_KD_MD\": \"CodeBert_KD_MD.csv\",\n",
        "    \"Mistral_AWQ_KD_MD\": \"Mistral_AWQ_KD_MD.csv\",\n",
        "    \"Roberta_HS_M\": \"Roberta_HS_M.csv\",\n",
        "}\n",
        "\n",
        "# Expected columns for analysis\n",
        "expected_columns = [\n",
        "    'Precision_Performance', 'Recall_Performance',\n",
        "    'Precision_Non_Performance', 'Recall_Non_Performance'\n",
        "]\n",
        "\n",
        "# Load, normalize, and check DataFrames\n",
        "dataframes = {}\n",
        "for name, path in file_paths.items():\n",
        "    df = pd.read_csv(path)\n",
        "    df = normalize_columns(df)\n",
        "    df = check_and_fill_columns(df, expected_columns)\n",
        "    dataframes[name] = df\n",
        "\n",
        "# Perform paired t-tests\n",
        "results = {}\n",
        "model_names = list(dataframes.keys())\n",
        "\n",
        "for i in range(len(model_names)):\n",
        "    for j in range(i + 1, len(model_names)):\n",
        "        model1_name = model_names[i]\n",
        "        model2_name = model_names[j]\n",
        "        model1_metrics = dataframes[model1_name]\n",
        "        model2_metrics = dataframes[model2_name]\n",
        "        pair_key = f\"{model1_name} vs {model2_name}\"\n",
        "        print(f\"Running T-Test: {pair_key}\")\n",
        "        results[pair_key] = perform_paired_ttest(model1_metrics, model2_metrics)\n",
        "\n",
        "# Display the results\n",
        "for pair, metrics in results.items():\n",
        "    print(f\"\\nComparison: {pair}\")\n",
        "    print(\"Performance Class Metrics:\")\n",
        "    for metric, values in metrics['performance'].items():\n",
        "        print(f\"{metric} - t-statistic: {values['t_stat']:.3f}, p-value: {values['p_value']:.3f}\")\n",
        "    print(\"Non-Performance Class Metrics:\")\n",
        "    for metric, values in metrics['non_performance'].items():\n",
        "        print(f\"{metric} - t-statistic: {values['t_stat']:.3f}, p-value: {values['p_value']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXaaJSbIE1xa",
        "outputId": "5dea1700-28b3-4fb5-cbd2-5582c9bcf2ca"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running T-Test: CodeBert_KD_M vs CodeBert_KD_MD\n",
            "Running T-Test: CodeBert_KD_M vs Mistral_AWQ_KD_MD\n",
            "Running T-Test: CodeBert_KD_M vs Roberta_HS_M\n",
            "Running T-Test: CodeBert_KD_MD vs Mistral_AWQ_KD_MD\n",
            "Running T-Test: CodeBert_KD_MD vs Roberta_HS_M\n",
            "Running T-Test: Mistral_AWQ_KD_MD vs Roberta_HS_M\n",
            "\n",
            "Comparison: CodeBert_KD_M vs CodeBert_KD_MD\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: -inf, p-value: 0.000\n",
            "Recall_Performance - t-statistic: -inf, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: -inf, p-value: 0.000\n",
            "Recall_Non_Performance - t-statistic: -inf, p-value: 0.000\n",
            "\n",
            "Comparison: CodeBert_KD_M vs Mistral_AWQ_KD_MD\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: -2.971, p-value: 0.041\n",
            "Recall_Performance - t-statistic: 20.156, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: 20.400, p-value: 0.000\n",
            "Recall_Non_Performance - t-statistic: -6.485, p-value: 0.003\n",
            "\n",
            "Comparison: CodeBert_KD_M vs Roberta_HS_M\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: inf, p-value: 0.000\n",
            "Recall_Performance - t-statistic: inf, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: inf, p-value: 0.000\n",
            "Recall_Non_Performance - t-statistic: inf, p-value: 0.000\n",
            "\n",
            "Comparison: CodeBert_KD_MD vs Mistral_AWQ_KD_MD\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: 3.790, p-value: 0.019\n",
            "Recall_Performance - t-statistic: 28.647, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: 31.838, p-value: 0.000\n",
            "Recall_Non_Performance - t-statistic: 1.669, p-value: 0.170\n",
            "\n",
            "Comparison: CodeBert_KD_MD vs Roberta_HS_M\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: inf, p-value: 0.000\n",
            "Recall_Performance - t-statistic: inf, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: inf, p-value: 0.000\n",
            "Recall_Non_Performance - t-statistic: inf, p-value: 0.000\n",
            "\n",
            "Comparison: Mistral_AWQ_KD_MD vs Roberta_HS_M\n",
            "Performance Class Metrics:\n",
            "Precision_Performance - t-statistic: 20.171, p-value: 0.000\n",
            "Recall_Performance - t-statistic: -10.604, p-value: 0.000\n",
            "Non-Performance Class Metrics:\n",
            "Precision_Non_Performance - t-statistic: -6.872, p-value: 0.002\n",
            "Recall_Non_Performance - t-statistic: 26.872, p-value: 0.000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
            "  res = hypotest_fun_out(*samples, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5m-UAK9KIC0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}